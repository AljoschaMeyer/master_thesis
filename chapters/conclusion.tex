% !TEX root = ../main.tex

We have presented range-based set synchronization, an approach that can reconcile two sets in $\complexity{\log(n)}$ communication rounds, transmitting $\complexity{\min(n_{\triangle} \cdot \log_b(n), n)}$ bits, and performing computations in each communication round requiring $\complexity{\min(k \cdot \log(\abs{S}), \abs{S})}$ time and $\complexity{1}$ space. Other approaches in the literature improve on the communication complexity by removing the logarithmic factor, but at the cost of at least $\complexity{n}$ time and $\complexity{n_{\triangle}}$ space for the per-round computations. The range-based approach is easily adapted to mirror a set, none of the other set reconciliation literature can be modified in this way.

Low computational complexity and bounded space usage are required for robust and scalable implementations, transmitting logarithmically more bits is still more effective in practice than running out of memory and crashing. We thus believe that range based synchronization should be part of the toolkit for building distributed systems just like the more complex reconciliation protocols. As an extreme example, a microcontroller connected to network-attached storage could perform reconciliation of large sets even though it has restrictive computational capabilities.

For maximized efficiency in the face of constrained computational resources, hybrid approaches can be considered. The range-based approach transmits range item sets if the number of items inside a range falls below a threshold $b$. The simple exchange of these items can be considered as a minor optimization on the trivial reconciliation protocol, but any other set reconciliation protocol could also be used. Range-based fingerprint comparisons can thus be used to find ranges containing less than $b$ items for both nodes, these ranges can then be reconciled using e.g. characteristic polynomial interpolation or Invertible Bloom Lookup Tables. Note however that if the space usage of the computations is to remain in $\complexity{1}$, these ``leaf'' reconciliations have to be performed sequentially, not in parallel.

\section{Future Work}

Beyond the scope of this thesis, there are some natural further questions to study. \Cref{crypto-general} lists some qualitative arguments for the validity of the approach even in the presence of a small number of adversarially created hash collisions. To give full confidence, a quantitative cryptographic analysis needs to answer questions such as the number of pairwise distinct collisions that can be adequately protected against through range boundary randomization, how well range boundary randomization can protect against $k$ different subsets all hashing to the same digest, and whether attacks against the collision resistance of the relevant monoids can be extended to produce such $k$-wise collisions.

We have considered the range-based approach for a two-party setting only, a natural generalization is that of multi-party set synchronization. This can of course be reduced to pairwise synchronization, but an interesting question is whether more efficient range-based protocols can be developed, similar to how~\cite{boral2014multi} adapts characteristic polynomial interpolation and~\cite{mitzenmacher2018simple} adapts invertible bloom lookup tables.

While we have studied the protocol from a theoretical perspective, there are also some software design issues that arise in the context of actual implementations. Consider for example one of the original motivating examples, an unordered peer-to-peer pubsub mechanism. A node in such a network would ideally perform set reconciliation as a first step in a session, and then eagerly forward new items as they became available. Thus new items might arrive in the middle of another reconciliation step, the same problem occurs when doing multiple reconciliation steps in parallel. The new items need to be buffered until reconciliation has finished, and then they can be transmitted to the peer. One solution is to implement the sets as persistent data structures, but are there more efficient mutable solutions based on locking? Or can the buffering of concurrently arriving items possibly avoid it altogether? We have provided the theoretical basis for robust, scalable systems based on set replication, but a practical implementation raises interesting questions in its own right.
