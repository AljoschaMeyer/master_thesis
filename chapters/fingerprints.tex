% !TEX root = ../main.tex

The protocols we will discuss work by recursively computing and comparing fingerprints of sets.
This chapter defines and motivates a specific fingerprinting scheme that admits efficient computation with small overhead for the storage and maintenance of auxiliary data structures. \Cref{initial-considerations} outlines the solution space and theoretic bounds. We examine randomized solutions that with high probability compute fingerprints in logarithmic time in \cref{randomization}. \Cref{group-fingerprints} characterizes a family of functions that admit efficient incremental computation, and \Cref{collisions} proposes members of this family that can be used for fingerprinting, and \cref{crypto} examine security concerns in the face of malicious parties trying to find fingerprint collisions.

%TODO: move the following definitions to where they are needed
%
%\begin{definition}
%Let $(U_0, \groupaddsym_0, \neutraladd_0)$ and $(U_1, \groupaddsym_1, \neutraladd_1)$ be monoids and let $fun{f}{U_0}{U_1}$. We call $\f$ a \defined{monoid homomorphism} if it satisfies two properties:
%
%\begin{description}
%  \item[preserves operation:] for all $x, y \in U_0$: $\f(x \groupaddsym_0 y) = \f(x) \groupaddsym_1 \f(y)$
%  \item[preserves neutral element:] $f(\neutraladd_0) = \neutraladd_1$
%\end{description}
%\end{definition}
%
%\begin{definition}
%Let $(U_0, \groupaddsym_0, \neutraladd_0)$ and $(U_1, \groupaddsym_1, \neutraladd_1)$ be monoids. The \defined{direct product of $(U_0, \groupaddsym_0, \neutraladd_0)$ and $(U_1, \groupaddsym_1, \neutraladd_)$} is the monoid $(U_0 \times U_1, \f, (\neutraladd_0, \neutraladd_1))$ with $\f((u_0, u_1), (v_0, v_1)) \defeq (u_0 \groupaddsym_0 v_0, u_1 \groupaddsym_1 v_1)$.
%\end{definition}

\section{Initial Considerations}
\label{initial-considerations}

Our protocols work by recursively testing fingerprints for equality. For our purposes, we can define a fingerprint or hash function as follows:

\begin{definition}
A \defined{hash function} is a function $\fun{\h}{U}{D}$ with a finite codomain such that for randomly chosen $u \in U$ and $d \in D$ the probability that $\h(u) = d$ is roughly\footnote{To keep the focus on data structure synchronization rather than being sidetracked by cryptography, we will for the most part keep arguments about probabilities qualitative rather than quantitative.} $\frac{1}{\abs{D}}$. $\h(u)$ is called the \defined{hash of $u$}, \defined{fingerprint of $u$} or \defined{digest of $u$}.
\end{definition}

Given a universe $U$ of items, a function $\enc : U \rightarrow \{0, 1\}^{*}$ for encoding items as binary strings, a linear order $\preceq$ on $U$, a hash function $\h: \{0, 1\}^{*} \rightarrow \{0, 1\}^{k}$ mapping binary strings to binary strings of length $k$ and some finite $S \subseteq U$, a natural starting point for defining a fingerprint of the set $S$ is to sort the items according to $\preceq$, concatenate the encodings, and hash the resulting string.

While this is straightforward to specify and implement, it does not suffice for our purposes. To allow for efficient set reconciliation, we need to be able to efficiently compute the new fingerprint after a small modification of the set such as insertion or deletion of a single item. Furthermore, we want to be able to efficiently compute the fingerprints of all subsets defined by an interval of the original set.

The fingerprint based on concatenating encodings does not allow for efficient incremental reevaluation. When an item is added to $S$ that is less than any item previously in $S$, the hash function needs to be run over the whole string of length $\complexity{\abs{S} + 1}$ again. Furthermore, for any subinterval of the set, a full fingerprint computation needs to be performed as well. Precomputing the fingerprints of all subintervals requires a prohibitive amount of space. Every subinterval corresponds to a substring of the string consisting of all items in $S$ in ascending order, so there are $\frac{\abs{S} \cdot (\abs{S} + 1)}{2} + 1 \in \complexity{n^2}$ many in total.

The go-to approach for efficiently handling small changes to a set of totally ordered items are (balanced) search trees, we briefly state some definitions.

\begin{definition}
Let $U$ be a set and $\preceq$ a binary relation on $U$.
We call $\preceq$ a \defined{linear order on $U$} if it satisfies three properties:

  \begin{description}
    \item[anti-symmetry:] for all $x, y \in U$: if $x \preceq y$ and $y \preceq x$ then $x = y$
    \item[transitivity:] for all $x, y, z \in U$: if $x \preceq y$ and $y \preceq z$ then $x \preceq z$
    \item[linearity:] for all $x, y \in U$: $x \preceq y$ or $y \preceq x$
  \end{description}

If $\preceq$ is a linear order, we write $x \prec y$ to denote that $x \preceq y$ and $x \neq y$.
\end{definition}

\begin{definition}
Let $U$ be a set, $\preceq$ a linear order on $U$, and $V \subseteq U$. Let $T$ be a rooted directed tree with vertex set $V$.

Let $v \in V$, then $T_v$ denotes the subtree of $T$ with root $v$.

$T$ is a \defined{binary search tree on V} if for all inner vertices $v$ with left child $a$ and right child $b$: $a' \prec v$ for all $a' \in T_a$ and $v \prec b'$ for all $b' \in T_b$.


\end{definition}

\begin{definition}
Let $T = (V, E)$ be a binary search tree and $\epsilon \in \mathbb{R}_{> 0}$.
We call $T$ \defined{$\epsilon$-balanced} if $\textit{height}(T) \leq \ceil*{\epsilon \cdot log_2(|V|)}$.
Since the precise choice of $\epsilon$ will not matter for our complexity analyses, we will usually simply talk about \defined{balanced} trees.
\end{definition}

In the context of fingerprinting, balanced trees often take the form of Merkle trees~\cite{merkle1989certified}, binary trees storing items in their leaves, in which each leaf vertex is labeled with the hash of the associated item, and inner vertices are labeled with the hash of the concatenation of the child labels. The root label serves as a fingerprint for the set of items stored in the leaves.

When inserting or removing an item, the number of labels that need updating is proportional to the length of of the path from the root to the item, so in a balanced tree of $n$ items $\complexity{\mathit{log}(n)}$. The problem with this approach however is that fingerprints are no longer unique: there are different balanced search trees storing the same items set, and different tree arrangements result in different root labels.

Unfortunately it does not suffice to specify a particular balancing scheme, since different insertion orders of the same overall set of items can result in different trees, even when using the same balancing scheme. While this is sufficient for a setting in which only a single machine updates the set and all other machines apply updates in the same order, as assumed e.g. in~\cite{nissim1998certificate}, we aim for a less restrictive setting in which the evolution of the local set does not influence synchronization.

An alternative would be to define exactly one valid tree shape for any set of items, but this precludes logarithmic time complexity of updates, as \cite{uniquerepresentation} shows that search, insertion and deletion in such trees take at least $\complexity{\sqrt{n}}$ time in the worst case.

We will inspect two options for cheating this lower bound and to achieve logarithmic complexity: utilizing randomization to define a unique tree layout which allows logarithmic operations with high probability, which we examine in \cref{randomization}, or letting the fingerprint function abstract over the tree shape, downgrading it to an implementation detail, which we examine in \cref{group-fingerprints} and beyond.

\section{Pseudorandom Data Structures}
\label{randomization}

TODO: hash tries, possibly skip lists

% tarjan set equality testing https://apps.dtic.mil/sti/pdfs/ADA223643.pdf
% pugh function caching data structures http://matthewhammer.org/courses/csci7000-s17/readings/Pugh89.pdf

\section{Incremental Computations}
\label{group-fingerprints}

We now study a family of fingerprinting functions for sets that admit efficient incremental computation based on an auxiliary tree structure, but whose output does not depend on the exact shape of that tree. We first examine a general class of such functions, which reduce a finite set to a single value according to a monoid.

\begin{definition}
Let $M$ be a set, $\groupaddsym: M \times M \rightarrow U$, and $\neutraladd \in M$.

We call $(U, \groupaddsym, \neutraladd)$ a \defined{monoid} if it satisfies two properties:

  \begin{description}
    \item[associativity:] for all $x, y, z \in M$: $\groupadd{(\groupadd{x}{y})}{z} = \groupadd{x}{\groupadd{y}{z}}$
    \item[neutral element:] for all $x \in M$: $\groupadd{\neutraladd}{x} = x = \groupadd{x}{\neutraladd}$.
  \end{description}
\end{definition}

\begin{definition}
Let $U$ be a set, $\preceq$ a linear order on $U$, $\mathcal{M} \defeq (M, \groupaddsym, \neutraladd)$ a monoid, and $\fun{\f}{U}{M}$.

We \defined{lift $\f$ to finite sets via $\mathcal{M}$} to obtain $\fun{\lift{\f}{\mathcal{M}}}{\powerset{U}}{M}$ with:

\begin{align*}
\lift{\f}{\mathcal{M}}(\emptyset) &\defeq \mymathbb{0}\\
\lift{\f}{\mathcal{M}}(S) &\defeq \f(\min_{\preceq}(S)) \oplus \lift{\f}{\mathcal{U}}(S \setminus \min_{\preceq}(S))\\
\end{align*}

In other words, if $S = \set{s_0, s_1, \ldots, s_{\abs{S} - 1}}$ with $s_0 \prec s_1 \prec \ldots \prec s_{\abs{S} - 1}$, then $\lift{\f}{\mathcal{M}}(S) = \groupadd{\f(s_0)}{\groupadd{\f(s_1)}{\groupadd{\ldots}{\f(s_{\abs{S} - 1})}}}$.
\end{definition}

Functions of the form $\lift{\f}{\mathcal{M}}$ can be incrementally computed by using labeled binary search trees:

\begin{definition}
Let $U$ be a set, $S \subset U$ a finite set, $\preceq$ a linear order on $U$, $\mathcal{M} \defeq (M, \groupaddsym, \neutraladd)$ a monoid, $\fun{\f}{U}{M}$, and let $T$ be a binary search tree on $S$.

We define a \defined{labeling function} $\fun{\liftlabel{\f}{\mathcal{M}}}{S}{M}$:

  \[
   \liftlabel{\f}{\mathcal{M}}(v) \defeq \begin{cases}
\f(v), &  \text{for leaf $v$} \\
\groupadd{\liftlabel{\f}{\mathcal{M}}(c_{<})}{\f(v)} & \, \text{v internal vertex with left child $c_{<}$ and no right child}\\
\groupadd{\f(v)}{\liftlabel{\f}{\mathcal{M}}(c_{<})} & \, \text{v internal vertex with right child $c_{>}$ and no left child}\\
\groupadd{\liftlabel{\f}{\mathcal{M}}(c_{<})}{\groupadd{\f(v)}{\liftlabel{\f}{\mathcal{M}}(c_{>})}} & \, \text{v internal vertex with left child $c_{<}$ and right child $c_{>}$}
\end{cases}
  \]

TODO fix overflow
See \cref{fig:fp-tree} for an example.
\end{definition}

\begin{figure*}
\begin{scaletikzpicturetowidth}{\textwidth}
\begin{tikzpicture}[scale=\tikzscale]
	\pgfdeclarelayer{background}
	\pgfdeclarelayer{foreground}
	\pgfsetlayers{background,main,foreground}
	
	\begin{pgfonlayer}{main}
		%vertices
		\node (vroot) at (0, 1) [aux] {\aux{\exampled}{
\groupadd{
    (\groupadd{\hexamplea}{\groupadd{\hexampleb}{\hexamplec)}}}
{\groupadd{\hexampled}
{(\groupadd{\hexamplee}{\groupadd{\hexamplef}{\hexampleg}})}
}
}};

		\node (v00) at (-4, -1) [aux] {\aux{\exampleb}{\groupadd{\hexamplea}{\groupadd{\hexampleb}{\hexamplec}}}};
		\node (v01) at (4, -1) [aux] {\aux{\examplef}{\groupadd{\hexamplee}{\groupadd{\hexamplef}{\hexampleg}}}};

                \node (v10) at (-6, -3) [aux] {\aux{\examplea}{\hexamplea}};
                \node (v11) at (-2, -3) [aux] {\aux{\examplec}{\hexamplec}};
                \node (v12) at (2, -3) [aux] {\aux{\examplee}{\hexamplee}};
                \node (v13) at (6, -3) [aux] {\aux{\exampleg}{\hexampleg}};
		%edges
                \draw (vroot) edge[edge] (v00);
                \draw (vroot) edge[edge] (v01);

		\draw (v00) edge[edge] (v10);
		\draw (v00) edge[edge] (v11);
		\draw (v01) edge[edge] (v12);
		\draw (v01) edge[edge] (v13);
	\end{pgfonlayer}
\end{tikzpicture}
\end{scaletikzpicturetowidth}

\caption{
A balanced search tree labeled by $\liftlabel{\h}{(M, \groupaddsym, \neutraladd)}$. For fingerprinting, $\h$ could be a hash function and $\groupaddsym$ the xor operation on fixed-width bitstrings.
}

\label{fig:fp-tree}
\end{figure*}

\begin{proposition}
Let $U$ be a set, $S \subset U$ a finite set, $\preceq$ a linear order on $U$, $\mathcal{M} \defeq (M, \groupaddsym, \neutraladd)$ a monoid, $\fun{\f}{U}{M}$, and let $T$ be a binary search tree on $S$ with root $r \in S$.

Then $\liftlabel{\f}{\mathcal{M}}(r) = \lift{\f}{\mathcal{M}}(S)$.

\begin{proof}
By induction on the number of vertices of $T$.

\textbf{IB:} If $r$ is a leaf, then $\abs{\V(T)} = 1$ and thus $\liftlabel{\f}{\mathcal{M}}(r) \overset{\text{def}}= \f(r) \overset{\text{def}}= \lift{\f}{\mathcal{M}}(\V(T)) = \lift{\f}{\mathcal{M}}(S)$.

\textbf{IH:} Let  $c_{<}$ and $c_{>}$ be vertices for which $\liftlabel{\f}{\mathcal{M}}(c_{<}) = \lift{\f}{\mathcal{M}}(\V(T_{c_{<}}))$ and $\liftlabel{\f}{\mathcal{M}}(c_{>}) = \lift{\f}{\mathcal{M}}(\V(T_{c_{>}}))$.

\textbf{IS:} If $r$ is an internal vertex with left child $c_{<}$ and right child $c_{>}$, then:

\begin{align*}
 \liftlabel{\f}{\mathcal{M}}(r) &\overset{\text{def}}= \groupadd{\liftlabel{\f}{\mathcal{M}}(c_{<})}{\groupadd{\f(r)}{\liftlabel{\f}{\mathcal{M}}(c_{>})}}\\
&\overset{\text{IH}}= \groupadd{\lift{\f}{\mathcal{M}}(\V(T_{c_{<}}))}{\groupadd{\f(p)}{\lift{\f}{\mathcal{M}}(\V(T_{c_{>}}))}}\\
& \overset{\text{def}}= \lift{\f}{\mathcal{M}}(\V(T))\\
&= \lift{\f}{\mathcal{M}}(S)\\
\end{align*}

The cases for internal vertices with exactly one child follow analogously.
\end{proof}
\end{proposition}

This correspondence can be used to incrementally compute $\lift{\f}{\mathcal{M}}(S)$: Initially, a labeled search tree storing the items in $S$ is constructed. $\lift{\f}{\mathcal{M}}(S)$ is the root label. When an item is inserted or removed, only the labels on the path from the root to the point of modification require recomputation, so only a logarithmic number of operations is performed if a self-balancing tree is used.

Note that the exact shape of the tree determines the grouping of how to apply $\groupaddsym$, but by associativity all groupings yield the same result. All trees storing the same set have the same root label.

If $U$ is small enough that space usage of $\complexity{\abs{U}}$ is acceptable, an implicit tree representation such as a binary indexed tree (Fenwick tree)~\cite{fenwick1994new} can be used. Array positions that correspond to some $u \in U \setminus S$ are simply filled with a dummy value whose hash is defined to be $\neutraladd$.

\subsection{Subsets}

In addition to incremental computation of the fingerprint of a given set, the reconciliation protocol also requires the efficient computation of the fingerprints of arbitrary intervals of the given set. We first fix some terminology and notation:

\begin{definition}
Let $S \subseteq U$, $\preceq$ a linear order over $U$, and $x, y \in U$.

The \defined{interval from $x$ to $y$ in $S$}, denoted by $\interval{x}{y}{S}$, is the set $\set{s \in S | x \preceq s \prec y}$. We call $x$ the \defined{lower boundary} and $y$ the \defined{upper boundary} of the interval.

Note that the upper boundary is excluded from the interval, so in particular $\interval{x}{x}{S} = \emptyset$.
\end{definition}

Given a balanced search tree $T$ with root $r$ for a set $S$ that is labeled by $\liftlabel{\f}{\mathcal{M}}$, we can compute $\lift{\f}{\mathcal{M}}(\interval{x}{y}{S})$ in logarithmic time. Intuitively, one traces paths in $T$ to both $x$ and $y$, and then the result is the sum over all vertices ``in the area between'' these paths. For every vertex on the traced paths, the label of the ``inner'' child vertex summarizes multiple vertices within the area. Summing over all these children yields the value corresponding to the whole inner area. Since the length of the delimiting paths is logarithmic, overall only a logarithmic number of labels needs to be added up.

\Cref{listing:subset-fingerprint} gives a precise definition of how to compute $\lift{\f}{\mathcal{M}}(\interval{x}{y}{S})$ as Haskell~98~\cite{jones2003haskell} code, for clarity of presentation only complete binary trees are considered. Arbitrary binary trees can also have inner nodes with exactly one child. These can be handled with almost the same algorithm by acting as if these nodes had a second child labeled with $\neutraladd$.

\begin{listing}
\begin{minted}[linenos, mathescape]{haskell}
-- $U$ is the type of items, $D$ the type of fingerprints.
-- A node is either a leaf or an inner vertex.
-- Both store a fingerprint as a label.
data Node = Leaf U D | Inner Node U Node D

-- Extracts the label of a node.
label :: Node -> D
label Leaf _ fp      = fp
label Inner _ _ _ fp = fp

-- Compute the fingerprint over all items stored in $v$
-- within the interval $\interval{x}{y}{S}$.
intervalFingerprint :: Node -> U -> U -> D
intervalFingerprint v x y = case findInitial v x y of
    Nothing               -> 0
    Just (Leaf _ fp)      -> fp
    Just (Inner l v' r _) -> (sumGeq l x) + (f v') + (sumLt r y)
                                           
-- Find the node within $\interval{x}{y}{S}$ that is closest to the root.
findInitial :: Node -> U -> U -> Maybe Node
findInitial (Leaf v _) x y
    | x <= v && v < y = Just v
    | otherwise       = Nothing
findInitial (Inner l v r _) x y
    | v < x           = findInitial r x y
    | v >= y          = findInitial l x y
    | otherwise       = Just v

-- Sum up the fingerprints of all items in the given tree
-- which are greater than or equal to $x$.
sumGeq :: Node -> U -> D
sumGeq (Leaf v fp) x
    | v < x     = 0
    | otherwise = fp
sumGeq (Inner l v r _) x
    | v < x     = sumGeq r x
    | otherwise = (sumGeq l x) + (f v) + (label r)

-- Sum up the fingerprints of all items in the given tree
-- which are strictly less than $y$.
sumLt :: Node -> U -> D
sumLt (Leaf v fp) y
    | v >= y    = 0
    | otherwise = fp
sumLt (Inner l v r _) y
    | v >= y    = sumLt l y
    | otherwise = (label l) + (f v) + (sumLt r y)
\end{minted}
\caption{Computing $\interval{x}{y}{S}$ from the complete labeled search tree of $S$.}
\label{listing:subset-fingerprint}
\end{listing}

The algorithm proceeds by first finding the vertex $v$ with the smallest distance to the root such that $x \preceq v \prec y$. This might be $r$ itself. If there is no such $v$, then $\interval{x}{y}{S} = \emptyset$ and thus $\lift{\f}{\mathcal{M}}(\interval{x}{y}{S}) = \neutraladd$. All vertices of $T$ that are not vertices in $T_v$ are either greater than or equal to $y$ if $v \prec`x$, or strictly less than $x$ if $x \prec v$, in either case they do not influence $\lift{\f}{\mathcal{M}}(\interval{x}{y}{S})$.

If $v$ is a leaf, $\interval{x}{y}{S} = \set{v}$ and thus $\lift{\f}{\mathcal{M}}(\interval{x}{y}{S}) = \f(v)$. Otherwise, let $c_{<}$ be the left child of $v$ and let $c_{>}$ be the right child. Since all vertices in $T_{c_{>}}$ are greater than $v$, they are in particular greater than $x$. Analogously all vertices in $T_{c_{<}}$ are less than $y$.

Keeping in mind that $\interval{x}{\max(\V(T_{c_{<}}))}{\V(T_{c_{<}})}$ simply denotes the set of vertices in $T_{c_{<}}$ that are strictly greater than $x$, and analogously $\interval{\min(\V(T_{c_{>}}))}{y}{\V(T_{c_{>}})}$ denoting the vertices of $T_{c_{>}}$ that are less than or equal to $y$, we have:

\begin{align*}
\interval{x}{y}{S} &= \disjointunion{\interval{x}{\max(\V(T_{c_{<}}))}{\V(T_{c_{<}})}}{\disjointunion{\set{v}}{\interval{\min(\V(T_{c_{>}}))}{y}{\V(T_{c_{>}})}}}\\
&\mathrm{implying}\\
\lift{\f}{\mathcal{M}}(\interval{x}{y}{S}) &= \groupadd{\lift{\f}{\mathcal{M}}(\interval{x}{\max(\V(T_{c_{<}}))}{\V(T_{c_{<}})})}{\groupadd{\f(v)}{\lift{\f}{\mathcal{M}}(\interval{\min(\V(T_{c_{>}}))}{y}{\V(T_{c_{>}})})}}\\
\end{align*}

Proving that \texttt{sumGeq} from \cref{listing:subset-fingerprint} does indeed sum over all $\f(v)$ in the given tree with $x \preceq v$, i.e. computes $\lift{\f}{\mathcal{M}}(\interval{x}{\max(\V(T_{c_{<}}))}{\V(T_{c_{<}})})$ can be done by a rather technical but straightforward induction which we omit, same for \texttt{sumLt} computing $\lift{\f}{\mathcal{M}}(\interval{\min(\V(T_{c_{>}}))}{y}{\V(T_{c_{>}})})$.

From those facts, correctness of \texttt{intervalFingerprintGroup} follows by the previous arguments.

The worst-case running time occurs when the vertex $v$ with the smallest distance to $r$ such that $x \preceq v \prec y$ is $r$ itself, since then both \texttt{sumGeq} and \texttt{sumLt} perform a traversal to a leaf of maximal length. Since $T$ is balanced, only a logarithmic number of recursive calls is executed. Assuming $\f$ can be computed in $\complexity{1}$, the resulting time complexity is in $\complexity{\log(\abs{S})}$.

A slightly simpler approach can be taken if $M$ has efficiently computable inverses with respect to $\groupaddsym$, i.e. if $(M, \groupaddsym, \neutraladd)$ is a group.

\begin{definition}
Let $(M, \groupaddsym, \neutraladd)$ be a monoid.
We call it a \defined{group} if for all $x \in M$ there exists $y \in M$ such that $\groupadd{x}{y} = \neutraladd$.
This $y$ is necessarily unique and denoted by $\inverseadd{x}$.
For $x, y \in M$ we write $\groupsubtract{x}{y}$ as a shorthand for $\groupadd{x}{\inverseadd{y}}$.
\end{definition}

Observe that $\interval{x}{y}{S} = \interval{\min(S)}{y}{S} \setminus \interval{\min(S)}{x}{S}$, and thus also $\lift{\f}{\mathcal{M}}(\interval{x}{y}{S}) = \groupadd{\inverseadd{\lift{\f}{\mathcal{M}}(\interval{\min(S)}{x}{S})}}{\lift{\f}{\mathcal{M}}(\interval{\min(S)}{y}{S})}$. Reusing the definitions from \cref{listing:subset-fingerprint}, we get:

\begin{minted}{haskell}
intervalFingerprintGroup :: Node -> U -> U -> D
intervalFingerprintGroup v x y = -(sumLt v x) + (sumLt v y)
\end{minted}

Alternatively, a slightly more efficient version that still does not require \texttt{sumGeq}:

\begin{minted}{haskell}
intervalFingerprint :: Node -> U -> U -> D
intervalFingerprint v x y = case findInitial v x y of
    Nothing               -> 0
    Just (Leaf _ fp)      -> fp
    Just (Inner l v' r _) -> -(sumLt l x) + (f v') + (sumLt r y)
\end{minted}

\section{Monoidal Fingerprints}
\label{collisions}

Now that we have characterized a family of functions that admit efficient recomputation in response to changes to the underlying set as well as efficient computation for intervals within the set, the remaining task is to find such functions which are also suitable fingerprints. This consists of deciding on the monoid of fingerprints, and choosing the mapping from items to monoid elements.

As the fingerprint of a singleton set $\lift{\f}{\mathcal{M}}(\set{u})$ is equal to $\f(u)$, $\f$ must itself already be a hash function. Typical hash functions map values to bit strings of a certain length, i.e. the codomain is $\set{0, 1}^k$ for some $k \in \N$. We will thus consider monoids whose elements can be represented by such bit strings.

A natural choice of the monoid universe is then $\interval{0}{2^k}{\N}$, some simple monoidical operations on this universe include bitwise xor, addition modulo $2^k$, and multiplication modulo $2^k$. In the following, addition and multiplication will always be implicitly taken modulo $2^k$. Note that $\xor$ and addition also admit inverses, so the slightly simplified computational fingerprints can be used.

Of these three options, multiplication is the least suitable, because multiplying $0$ by any number again yields $0$. Consequently for every set containing an item $u$ with $\f(u) = 0$ the fingerprint of the set would also be $0$, which clearly violates the criterion that all possible values for fingerprints occur with equal probability.

Addition and xor however are particularly well-behaved in that regard, as they form finite commutative groups:

\begin{proposition}
Let $\mathcal{G} \defeq (G, \groupaddsym, \neutraladd)$ be a finite commutative group, i.e. a group with a finite universe such that for all $x, y \in G$: $\groupadd{x}{y} = \groupadd{y}{x}$. Let $U$ be a set and let $\fun{f}{U}{G}$ be a hash function.

Then $\lift{\f}{\mathcal{G}}$ is a hash function as well.

\begin{proof}
We first show that for randomly chosen $x, y, z \in G$ the probability that $\groupadd{x}{y} = z$ is $\frac{1}{\abs{G}}$.

For $x, z \in G$ there is $y \in G$ such that $\groupadd{x}{y} = z$, namely $y \defeq \groupsubtract{z}{x}$ (because $\groupadd{x}{\groupsubtract{z}{x}} \overset{\text{commutativity}}= \groupadd{\groupadd{x}{\inverseadd{x}}}{z} = z$). As $G$ is finite, this $y$ has to be unique, since otherwise that would not be enough elements left that can be added to $x$ to result all of the $\abs{G} - 1$ possible remaining $z'$. Thus for any fixed $x, z$ the probability that a randomly chosen $y$ satisfies $\groupadd{x}{y} = z$ is $\frac{1}{\abs{G}}$.

Computing $\lift{\f}{\mathcal{G}}(S)$ consists of repeatedly adding group elements which by themselves are distributed uniformly at random if $S$ was chosen randomly and $\f$ is a high quality hash function. Thus the accumulated value after every step is any given $z \in G$ with probability $\frac{1}{\abs{G}}$, as is in particular the probability for the result being equal to $z$.
\end{proof}
\end{proposition}

By the same argument, knowing the fingerprint for some set does not provide any information about the fingerprints for sets that differ by even only a single value. In conclusion, high quality fingerprints can be achieved by choosing any transitive and commutative operation for the monoid, for example xor or addition modulo $2^k$, as long as values are mapped into the monoid with a high quality hash function.

\section{Cryptographically Secure Fingerprints}
\label{crypto}

In the protocols for synchronizing data structures, fingerprints of sets are used for probabilistic equality checking: sets with equal fingerprints are assumed to be equal. Synchronization can thus become faulty if it involves unequal sets with equal fingerprints. If the universe of possible fingerprints is chosen large enough, and the distribution of fingerprints of randomly chosen sets is random within that universe, the probability for this occurring becomes negligible.

Random distribution of input sets is however a very strong assumption. What if a malicious party can influence the sets to be fingerprinted, with the goal of causing fingerprint collisions and consequently triggering faulty behavior of the system? Cryptographically secure fingerprints are an answer to this problem, being chosen such that it is computationally infeasible for an adversary to find inputs that lead to faulty synchronization.

\subsection{General Considerations}

Traditionally (TODO citation needed), cryptographically secure hash functions are defined as follows:

\begin{definition}
A \defined{secure hash function} is a hash function $\fun{\h}{U}{D}$ that satisfies two additional properties:

\begin{description}
  \item[pre-image resistance:] Given $d \in D$, it is computationally infeasible to find a $u \in U$ such that $\h(u) = d$.
  \item[collision resistance:] It is computationally infeasible to find $u, v \in U, u ~= v$ such that $\h(u) = \h(v)$.
\end{description}
\end{definition}

Pre-image resistance has no influence on the vulnerability of the protocol to malicious actors, so all of the following discussion will focus on collision resistance only.

Since $\lift{\f}{\mathcal{M}}(\set{u}) = \f(u)$, $\f$ must necessarily be collision resistant if $\lift{\f}{\mathcal{M}}$ is to be collision resistant. This alone is unfortunately not sufficient, we will see a specific counterexamples in the following subsections. Choosing a secure hash function $\f$ always comes with a performance cost, insecure hash functions usually take less time and less space to compute. If the synchronization protocol is only being run in a trusted environment, an insecure hash function might be preferable.

Whether a hash function is secure is not a binary dichotomy, but depends on what is considered ``feasible'' for an adversary. Greater security can usually be obtained at the cost of longer digests and longer computation times. Before presenting options for secure hash functions, we thus examine the impact of hash collisions first.

We can generally distinguish between malicious actors in two different positions: those who can actively impact the contents of the data structure to be synchronized, and those who passively relay updates and needs to search for a collision within the available data.

A passive malicious actor 




















































