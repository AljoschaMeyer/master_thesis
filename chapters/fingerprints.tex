% !TEX root = ../main.tex

The protocols we will discuss work by recursively computing and comparing fingerprints of sets.
This chapter defines and motivates a specific fingerprinting scheme that admits efficient computation with small overhead for the storage and maintenance of auxiliary data structures. \Cref{initial-considerations} outlines the solution space and theoretic bounds. We examine randomized solutions that with high probability compute fingerprints in logarithmic time in \cref{randomization}. \Cref{group-fingerprints} characterizes a family of functions that admit efficient incremental computation, and \Cref{collisions} proposes members of this family that can be used for fingerprinting, and \cref{crypto} examine security concerns in the face of malicious parties trying to find fingerprint collisions.

%TODO: move the following definitions to where they are needed
%
%\begin{definition}
%Let $(U_0, \groupaddsym_0, \neutraladd_0)$ and $(U_1, \groupaddsym_1, \neutraladd_1)$ be monoids and let $fun{f}{U_0}{U_1}$. We call $\f$ a \defined{monoid homomorphism} if it satisfies two properties:
%
%\begin{description}
%  \item[preserves operation:] for all $x, y \in U_0$: $\f(x \groupaddsym_0 y) = \f(x) \groupaddsym_1 \f(y)$
%  \item[preserves neutral element:] $f(\neutraladd_0) = \neutraladd_1$
%\end{description}
%\end{definition}
%
%\begin{definition}
%Let $(U_0, \groupaddsym_0, \neutraladd_0)$ and $(U_1, \groupaddsym_1, \neutraladd_1)$ be monoids. The \defined{direct product of $(U_0, \groupaddsym_0, \neutraladd_0)$ and $(U_1, \groupaddsym_1, \neutraladd_)$} is the monoid $(U_0 \times U_1, \f, (\neutraladd_0, \neutraladd_1))$ with $\f((u_0, u_1), (v_0, v_1)) \defeq (u_0 \groupaddsym_0 v_0, u_1 \groupaddsym_1 v_1)$.
%\end{definition}

\section{Initial Considerations}
\label{initial-considerations}

Our protocols work by recursively testing fingerprints for equality. For our purposes, we can define a fingerprint or hash function as follows:

\begin{definition}
A \defined{hash function} is a function $\fun{\h}{U}{D}$ with a finite codomain such that for randomly chosen $u \in U$ and $d \in D$ the probability that $\h(u) = d$ is roughly\footnote{To keep the focus on data structure synchronization rather than being sidetracked by cryptography, we will for the most part keep arguments about probabilities qualitative rather than quantitative.} $\frac{1}{\abs{D}}$. $\h(u)$ is called the \defined{hash of $u$}, \defined{fingerprint of $u$} or \defined{digest of $u$}.
\end{definition}

Given a universe $U$ of items, a function $\enc : U \rightarrow \{0, 1\}^{*}$ for encoding items as binary strings, a linear order $\preceq$ on $U$, a hash function $\h: \{0, 1\}^{*} \rightarrow \{0, 1\}^{k}$ mapping binary strings to binary strings of length $k$ and some finite $S \subseteq U$, a natural starting point for defining a fingerprint of the set $S$ is to sort the items according to $\preceq$, concatenate the encodings, and hash the resulting string.

While this is straightforward to specify and implement, it does not suffice for our purposes. To allow for efficient set reconciliation, we need to be able to efficiently compute the new fingerprint after a small modification of the set such as insertion or deletion of a single item. Furthermore, we want to be able to efficiently compute the fingerprints of all subsets defined by an interval of the original set.

The fingerprint based on concatenating encodings does not allow for efficient incremental reevaluation. When an item is added to $S$ that is less than any item previously in $S$, the hash function needs to be run over the whole string of length $\complexity{\abs{S} + 1}$ again. Furthermore, for any subinterval of the set, a full fingerprint computation needs to be performed as well. Precomputing the fingerprints of all subintervals requires a prohibitive amount of space. Every subinterval corresponds to a substring of the string consisting of all items in $S$ in ascending order, so there are $\frac{\abs{S} \cdot (\abs{S} + 1)}{2} + 1 \in \complexity{n^2}$ many in total.

The go-to approach for efficiently handling small changes to a set of totally ordered items are (balanced) search trees, we briefly state some definitions.

\begin{definition}
Let $U$ be a set and $\preceq$ a binary relation on $U$.
We call $\preceq$ a \defined{linear order on $U$} if it satisfies three properties:

  \begin{description}
    \item[anti-symmetry:] for all $x, y \in U$: if $x \preceq y$ and $y \preceq x$ then $x = y$
    \item[transitivity:] for all $x, y, z \in U$: if $x \preceq y$ and $y \preceq z$ then $x \preceq z$
    \item[linearity:] for all $x, y \in U$: $x \preceq y$ or $y \preceq x$
  \end{description}

If $\preceq$ is a linear order, we write $x \prec y$ to denote that $x \preceq y$ and $x \neq y$.
\end{definition}

\begin{definition}
Let $U$ be a set, $\preceq$ a linear order on $U$, and $V \subseteq U$. Let $T$ be a rooted directed tree with vertex set $V$.

Let $v \in V$, then $T_v$ denotes the subtree of $T$ with root $v$.

$T$ is a \defined{binary search tree on V} if for all inner vertices $v$ with left child $a$ and right child $b$: $a' \prec v$ for all $a' \in T_a$ and $v \prec b'$ for all $b' \in T_b$.


\end{definition}

\begin{definition}
Let $T = (V, E)$ be a binary search tree and $\epsilon \in \mathbb{R}_{> 0}$.
We call $T$ \defined{$\epsilon$-balanced} if $\textit{height}(T) \leq \ceil*{\epsilon \cdot log_2(|V|)}$.
Since the precise choice of $\epsilon$ will not matter for our complexity analyses, we will usually simply talk about \defined{balanced} trees.
\end{definition}

In the context of fingerprinting, balanced trees often take the form of Merkle trees~\cite{merkle1989certified}, binary trees storing items in their leaves, in which each leaf vertex is labeled with the hash of the associated item, and inner vertices are labeled with the hash of the concatenation of the child labels. The root label serves as a fingerprint for the set of items stored in the leaves.

When inserting or removing an item, the number of labels that need updating is proportional to the length of of the path from the root to the item, so in a balanced tree of $n$ items $\complexity{\mathit{log}(n)}$. The problem with this approach however is that fingerprints are no longer unique: there are different balanced search trees storing the same items set, and different tree arrangements result in different root labels.

Unfortunately it does not suffice to specify a particular balancing scheme, since different insertion orders of the same overall set of items can result in different trees, even when using the same balancing scheme. While this is sufficient for a setting in which only a single machine updates the set and all other machines apply updates in the same order, as assumed e.g. in~\cite{nissim1998certificate}, we aim for a less restrictive setting in which the evolution of the local set does not influence synchronization.

An alternative would be to define exactly one valid tree shape for any set of items, but this precludes logarithmic time complexity of updates, as \cite{uniquerepresentation} shows that search, insertion and deletion in such trees take at least $\complexity{\sqrt{n}}$ time in the worst case.

We will inspect two options for cheating this lower bound and to achieve logarithmic complexity: utilizing randomization to define a unique tree layout which allows logarithmic operations with high probability, which we examine in \cref{randomization}, or letting the fingerprint function abstract over the tree shape, downgrading it to an implementation detail, which we examine in \cref{group-fingerprints} and beyond.

\section{Pseudorandom Data Structures}
\label{randomization}

TODO: hash tries, possibly skip lists

% tarjan set equality testing https://apps.dtic.mil/sti/pdfs/ADA223643.pdf
% pugh function caching data structures http://matthewhammer.org/courses/csci7000-s17/readings/Pugh89.pdf

\section{Incremental Computations}
\label{group-fingerprints}

We now study a family of fingerprinting functions for sets that admit efficient incremental computation based on an auxiliary tree structure, but whose output does not depend on the exact shape of that tree. We first examine a general class of such functions, which reduce a finite set to a single value according to a monoid.

\begin{definition}
Let $M$ be a set, $\groupaddsym: M \times M \rightarrow U$, and $\neutraladd \in M$.

We call $(U, \groupaddsym, \neutraladd)$ a \defined{monoid} if it satisfies two properties:

  \begin{description}
    \item[associativity:] for all $x, y, z \in M$: $\groupadd{(\groupadd{x}{y})}{z} = \groupadd{x}{\groupadd{y}{z}}$
    \item[neutral element:] for all $x \in M$: $\groupadd{\neutraladd}{x} = x = \groupadd{x}{\neutraladd}$.
  \end{description}
\end{definition}

\begin{definition}
\label{def-lift}
Let $U$ be a set, $\preceq$ a linear order on $U$, $\mathcal{M} \defeq (M, \groupaddsym, \neutraladd)$ a monoid, and $\fun{\f}{U}{M}$.

We \defined{lift $\f$ to finite sets via $\mathcal{M}$} to obtain $\fun{\lift{\f}{\mathcal{M}}}{\powerset{U}}{M}$ with:

\begin{align*}
\lift{\f}{\mathcal{M}}(\emptyset) &\defeq \mymathbb{0}\\
\lift{\f}{\mathcal{M}}(S) &\defeq \f(\min_{\preceq}(S)) \oplus \lift{\f}{\mathcal{U}}(S \setminus \min_{\preceq}(S))\\
\end{align*}

In other words, if $S = \set{s_0, s_1, \ldots, s_{\abs{S} - 1}}$ with $s_0 \prec s_1 \prec \ldots \prec s_{\abs{S} - 1}$, then $\lift{\f}{\mathcal{M}}(S) = \groupadd{\f(s_0)}{\groupadd{\f(s_1)}{\groupadd{\ldots}{\f(s_{\abs{S} - 1})}}}$.
\end{definition}

Functions of the form $\lift{\f}{\mathcal{M}}$ can be incrementally computed by using labeled binary search trees:

\begin{definition}
Let $U$ be a set, $S \subset U$ a finite set, $\preceq$ a linear order on $U$, $\mathcal{M} \defeq (M, \groupaddsym, \neutraladd)$ a monoid, $\fun{\f}{U}{M}$, and let $T$ be a binary search tree on $S$.

We define a \defined{labeling function} $\fun{\liftlabel{\f}{\mathcal{M}}}{S}{M}$:

  \[
   \liftlabel{\f}{\mathcal{M}}(v) \defeq \begin{cases}
\f(v), &  \text{for leaf $v$} \\
\groupadd{\liftlabel{\f}{\mathcal{M}}(c_{<})}{\f(v)} & \, \text{v internal vertex with left child $c_{<}$ and no right child}\\
\groupadd{\f(v)}{\liftlabel{\f}{\mathcal{M}}(c_{<})} & \, \text{v internal vertex with right child $c_{>}$ and no left child}\\
\groupadd{\liftlabel{\f}{\mathcal{M}}(c_{<})}{\groupadd{\f(v)}{\liftlabel{\f}{\mathcal{M}}(c_{>})}} & \, \text{v internal vertex with left child $c_{<}$ and right child $c_{>}$}
\end{cases}
  \]

TODO fix overflow
See \cref{fig:fp-tree} for an example.
\end{definition}

\begin{figure*}
\begin{scaletikzpicturetowidth}{\textwidth}
\begin{tikzpicture}[scale=\tikzscale]
	\pgfdeclarelayer{background}
	\pgfdeclarelayer{foreground}
	\pgfsetlayers{background,main,foreground}
	
	\begin{pgfonlayer}{main}
		%vertices
		\node (vroot) at (0, 1) [aux] {\aux{\exampled}{
\groupadd{
    (\groupadd{\hexamplea}{\groupadd{\hexampleb}{\hexamplec)}}}
{\groupadd{\hexampled}
{(\groupadd{\hexamplee}{\groupadd{\hexamplef}{\hexampleg}})}
}
}};

		\node (v00) at (-4, -1) [aux] {\aux{\exampleb}{\groupadd{\hexamplea}{\groupadd{\hexampleb}{\hexamplec}}}};
		\node (v01) at (4, -1) [aux] {\aux{\examplef}{\groupadd{\hexamplee}{\groupadd{\hexamplef}{\hexampleg}}}};

                \node (v10) at (-6, -3) [aux] {\aux{\examplea}{\hexamplea}};
                \node (v11) at (-2, -3) [aux] {\aux{\examplec}{\hexamplec}};
                \node (v12) at (2, -3) [aux] {\aux{\examplee}{\hexamplee}};
                \node (v13) at (6, -3) [aux] {\aux{\exampleg}{\hexampleg}};
		%edges
                \draw (vroot) edge[edge] (v00);
                \draw (vroot) edge[edge] (v01);

		\draw (v00) edge[edge] (v10);
		\draw (v00) edge[edge] (v11);
		\draw (v01) edge[edge] (v12);
		\draw (v01) edge[edge] (v13);
	\end{pgfonlayer}
\end{tikzpicture}
\end{scaletikzpicturetowidth}

\caption{
A balanced search tree labeled by $\liftlabel{\h}{(M, \groupaddsym, \neutraladd)}$. For fingerprinting, $\h$ could be a hash function and $\groupaddsym$ the xor operation on fixed-width bitstrings.
}

\label{fig:fp-tree}
\end{figure*}

\begin{proposition}
Let $U$ be a set, $S \subset U$ a finite set, $\preceq$ a linear order on $U$, $\mathcal{M} \defeq (M, \groupaddsym, \neutraladd)$ a monoid, $\fun{\f}{U}{M}$, and let $T$ be a binary search tree on $S$ with root $r \in S$.

Then $\liftlabel{\f}{\mathcal{M}}(r) = \lift{\f}{\mathcal{M}}(S)$.

\begin{proof}
By induction on the number of vertices of $T$.

\textbf{IB:} If $r$ is a leaf, then $\abs{\V(T)} = 1$ and thus $\liftlabel{\f}{\mathcal{M}}(r) \overset{\text{def}}= \f(r) \overset{\text{def}}= \lift{\f}{\mathcal{M}}(\V(T)) = \lift{\f}{\mathcal{M}}(S)$.

\textbf{IH:} Let  $c_{<}$ and $c_{>}$ be vertices for which $\liftlabel{\f}{\mathcal{M}}(c_{<}) = \lift{\f}{\mathcal{M}}(\V(T_{c_{<}}))$ and $\liftlabel{\f}{\mathcal{M}}(c_{>}) = \lift{\f}{\mathcal{M}}(\V(T_{c_{>}}))$.

\textbf{IS:} If $r$ is an internal vertex with left child $c_{<}$ and right child $c_{>}$, then:

\begin{align*}
 \liftlabel{\f}{\mathcal{M}}(r) &\overset{\text{def}}= \groupadd{\liftlabel{\f}{\mathcal{M}}(c_{<})}{\groupadd{\f(r)}{\liftlabel{\f}{\mathcal{M}}(c_{>})}}\\
&\overset{\text{IH}}= \groupadd{\lift{\f}{\mathcal{M}}(\V(T_{c_{<}}))}{\groupadd{\f(p)}{\lift{\f}{\mathcal{M}}(\V(T_{c_{>}}))}}\\
& \overset{\text{def}}= \lift{\f}{\mathcal{M}}(\V(T))\\
&= \lift{\f}{\mathcal{M}}(S)\\
\end{align*}

The cases for internal vertices with exactly one child follow analogously.
\end{proof}
\end{proposition}

This correspondence can be used to incrementally compute $\lift{\f}{\mathcal{M}}(S)$: Initially, a labeled search tree storing the items in $S$ is constructed. $\lift{\f}{\mathcal{M}}(S)$ is the root label. When an item is inserted or removed, only the labels on the path from the root to the point of modification require recomputation, so only a logarithmic number of operations is performed if a self-balancing tree is used.

Note that the exact shape of the tree determines the grouping of how to apply $\groupaddsym$, but by associativity all groupings yield the same result. All trees storing the same set have the same root label.

If $U$ is small enough that space usage of $\complexity{\abs{U}}$ is acceptable, an implicit tree representation such as a binary indexed tree (Fenwick tree)~\cite{fenwick1994new} can be used. Array positions that correspond to some $u \in U \setminus S$ are simply filled with a dummy value whose hash is defined to be $\neutraladd$.

\subsection{Subsets}

In addition to incremental computation of the fingerprint of a given set, the reconciliation protocol also requires the efficient computation of the fingerprints of arbitrary intervals of the given set. We first fix some terminology and notation:

\begin{definition}
\label{def-interval}
Let $S \subseteq U$, $\preceq$ a linear order over $U$, and $x, y \in U$.

The \defined{interval from $x$ to $y$ in $S$}, denoted by $\interval{x}{y}{S}$, is the set $\set{s \in S \mid x \preceq s \prec y}$ if $x \prec y$, and $S \setminus \interval{y}{x}{S}$ if $y \preceq x$. We call $x$ the \defined{lower boundary} and $y$ the \defined{upper boundary} of the interval (even if $y \preceq x$).

Note that the upper boundary is excluded from the interval, so in particular $\interval{x}{x}{S} = S$.
\end{definition}

In the remainder of this section, we assume that for all intervals $\interval{x}{y}{S}$ we have $x \prec y$. If that is not the case, all computations can be performed for the sets $\set{s \in S \mid x \preceq s}$ and $\set{s \in S \mid s \prec y}$ which partition $\interval{x}{y}{S}$ if $y \preceq x$, and the resulting values can be combined via $\groupaddsym$ to obtain the desired result.

Given a balanced search tree $T$ with root $r$ for a set $S$ that is labeled by $\liftlabel{\f}{\mathcal{M}}$, we can compute $\lift{\f}{\mathcal{M}}(\interval{x}{y}{S})$ in logarithmic time. Intuitively, one traces paths in $T$ to both $x$ and $y$, and then the result is the sum over all vertices ``in the area between'' these paths. For every vertex on the traced paths, the label of the ``inner'' child vertex summarizes multiple vertices within the area. Summing over all these children yields the value corresponding to the whole inner area. Since the length of the delimiting paths is logarithmic, overall only a logarithmic number of labels needs to be added up.

\Cref{listing:subset-fingerprint} gives a precise definition of how to compute $\lift{\f}{\mathcal{M}}(\interval{x}{y}{S})$ as Haskell~98~\cite{jones2003haskell} code, for clarity of presentation only complete binary trees are considered. Arbitrary binary trees can also have inner nodes with exactly one child. These can be handled with almost the same algorithm by acting as if these nodes had a second child labeled with $\neutraladd$.

\begin{listing}
\begin{minted}[linenos, mathescape]{haskell}
-- $\mathtt{U}$ is the type of items, $\mathtt{D}$ the type of fingerprints.
-- A node is either a leaf or an inner vertex.
data Node = Leaf U D | Inner Node U Node D

-- Extracts the label of a node.
label :: Node -> D
label Leaf _ fp      = fp
label Inner _ _ _ fp = fp

-- Compute the fingerprint over all items stored in $v$
-- within the interval $\interval{x}{y}{S}$, assuming $x \prec y$.
intervalFingerprint :: Node -> U -> U -> D
intervalFingerprint v x y = case findInitial v x y of
    Nothing               -> 0
    Just (Leaf _ fp)      -> fp
    Just (Inner l v' r _) -> (sumGeq l x) + (f v') + (sumLt r y)
                                           
-- Find the node within $\interval{x}{y}{S}$ that is closest to the root,
-- assuming $x \prec y$.
findInitial :: Node -> U -> U -> Maybe Node
findInitial (Leaf v _) x y
    | x <= v && v < y = Just v
    | otherwise       = Nothing
findInitial (Inner l v r _) x y
    | v < x           = findInitial r x y
    | v >= y          = findInitial l x y
    | otherwise       = Just v

-- Sum up the fingerprints of all items in the given tree
-- which are greater than or equal to $x$.
sumGeq :: Node -> U -> D
sumGeq (Leaf v fp) x
    | v < x     = 0
    | otherwise = fp
sumGeq (Inner l v r _) x
    | v < x     = sumGeq r x
    | otherwise = (sumGeq l x) + (f v) + (label r)

-- Sum up the fingerprints of all items in the given tree
-- which are strictly less than $y$.
sumLt :: Node -> U -> D
sumLt (Leaf v fp) y
    | v >= y    = 0
    | otherwise = fp
sumLt (Inner l v r _) y
    | v >= y    = sumLt l y
    | otherwise = (label l) + (f v) + (sumLt r y)
\end{minted}
\caption{Computing $\interval{x}{y}{S}$ from the complete labeled search tree of $S$.}
\label{listing:subset-fingerprint}
\end{listing}

The algorithm proceeds by first finding the vertex $v$ with the smallest distance to the root such that $x \preceq v \prec y$. This might be $r$ itself. If there is no such $v$, then $\interval{x}{y}{S} = \emptyset$ and thus $\lift{\f}{\mathcal{M}}(\interval{x}{y}{S}) = \neutraladd$. All vertices of $T$ that are not vertices in $T_v$ are either greater than or equal to $y$ if $v \prec x$, or strictly less than $x$ if $x \prec v$, in either case they do not influence $\lift{\f}{\mathcal{M}}(\interval{x}{y}{S})$.

If $v$ is a leaf, $\interval{x}{y}{S} = \set{v}$ and thus $\lift{\f}{\mathcal{M}}(\interval{x}{y}{S}) = \f(v)$. Otherwise, let $c_{<}$ be the left child of $v$ and let $c_{>}$ be the right child. Since all vertices in $T_{c_{>}}$ are greater than $v$, they are in particular greater than $x$. Analogously all vertices in $T_{c_{<}}$ are less than $y$.

Keeping in mind that $\interval{x}{\max(\V(T_{c_{<}}))}{\V(T_{c_{<}})}$ simply denotes the set of vertices in $T_{c_{<}}$ that are strictly greater than $x$, and analogously $\interval{\min(\V(T_{c_{>}}))}{y}{\V(T_{c_{>}})}$ denotes the vertices of $T_{c_{>}}$ that are less than or equal to $y$, we have:

\begin{align*}
\interval{x}{y}{S} &= \disjointunion{\interval{x}{\max(\V(T_{c_{<}}))}{\V(T_{c_{<}})}}{\disjointunion{\set{v}}{\interval{\min(\V(T_{c_{>}}))}{y}{\V(T_{c_{>}})}}}\\
&\mathrm{implying}\\
\lift{\f}{\mathcal{M}}(\interval{x}{y}{S}) &= \groupadd{\lift{\f}{\mathcal{M}}(\interval{x}{\max(\V(T_{c_{<}}))}{\V(T_{c_{<}})})}{\groupadd{\f(v)}{\lift{\f}{\mathcal{M}}(\interval{\min(\V(T_{c_{>}}))}{y}{\V(T_{c_{>}})})}}\\
\end{align*}

Proving that \texttt{sumGeq} from \cref{listing:subset-fingerprint} does indeed sum over all $\f(v)$ in the given tree with $x \preceq v$, i.e. computes $\lift{\f}{\mathcal{M}}(\interval{x}{\max(\V(T_{c_{<}}))}{\V(T_{c_{<}})})$ can be done by a rather technical but straightforward induction which we omit, same for \texttt{sumLt} computing $\lift{\f}{\mathcal{M}}(\interval{\min(\V(T_{c_{>}}))}{y}{\V(T_{c_{>}})})$.

From those facts, correctness of \texttt{intervalFingerprintGroup} follows by the previous arguments.

The worst-case running time occurs when the vertex $v$ with the smallest distance to $r$ such that $x \preceq v \prec y$ is $r$ itself, since then both \texttt{sumGeq} and \texttt{sumLt} perform a traversal to a leaf of maximal length. Since $T$ is balanced, only a logarithmic number of recursive calls is executed. Assuming $\f$ can be computed in $\complexity{1}$, the resulting time complexity is in $\complexity{\log(\abs{S})}$.

A slightly simpler approach can be taken if $M$ has efficiently computable inverses with respect to $\groupaddsym$, i.e. if $(M, \groupaddsym, \neutraladd)$ is a group.

\begin{definition}
Let $(M, \groupaddsym, \neutraladd)$ be a monoid.
We call it a \defined{group} if for all $x \in M$ there exists $y \in M$ such that $\groupadd{x}{y} = \neutraladd$.
This $y$ is necessarily unique and denoted by $\inverseadd{x}$.
For $x, y \in M$ we write $\groupsubtract{x}{y}$ as a shorthand for $\groupadd{x}{\inverseadd{y}}$.
\end{definition}

Observe that $\interval{x}{y}{S} = \interval{\min(S)}{y}{S} \setminus \interval{\min(S)}{x}{S}$, and thus also $\lift{\f}{\mathcal{M}}(\interval{x}{y}{S}) = \groupadd{\inverseadd{\lift{\f}{\mathcal{M}}(\interval{\min(S)}{x}{S})}}{\lift{\f}{\mathcal{M}}(\interval{\min(S)}{y}{S})}$. Reusing the definitions from \cref{listing:subset-fingerprint}, we get:

\begin{minted}{haskell}
intervalFingerprintGroup :: Node -> U -> U -> D
intervalFingerprintGroup v x y = -(sumLt v x) + (sumLt v y)
\end{minted}

Alternatively, a slightly more efficient version that still does not require \texttt{sumGeq}:

\begin{minted}{haskell}
intervalFingerprint :: Node -> U -> U -> D
intervalFingerprint v x y = case findInitial v x y of
    Nothing               -> 0
    Just (Leaf _ fp)      -> fp
    Just (Inner l v' r _) -> -(sumLt l x) + (f v') + (sumLt r y)
\end{minted}

\section{Monoidal Fingerprints}
\label{collisions}

Now that we have characterized a family of functions that admit efficient recomputation in response to changes to the underlying set as well as efficient computation for intervals within the set, the remaining task is to find such functions which are also suitable fingerprints. This consists of deciding on the monoid of fingerprints, and choosing the mapping from items to monoid elements.

As the fingerprint of a singleton set $\lift{\f}{\mathcal{M}}(\set{u})$ is equal to $\f(u)$, $\f$ must itself already be a hash function. Typical hash functions map values to bit strings of a certain length, i.e. the codomain is $\set{0, 1}^k$ for some $k \in \N$. We will thus consider monoids whose elements can be represented by such bit strings.

A natural choice of the monoid universe is then $\interval{0}{2^k}{\N}$, some simple monoidical operations on this universe include bitwise xor, addition modulo $2^k$, and multiplication modulo $2^k$. In the following, addition and multiplication will always be implicitly taken modulo $2^k$. Note that $\xor$ and addition also admit inverses, so the slightly simplified computational fingerprints can be used.

Of these three options, multiplication is the least suitable, because multiplying $0$ by any number again yields $0$. Consequently for every set containing an item $u$ with $\f(u) = 0$ the fingerprint of the set would also be $0$, which clearly violates the criterion that all possible values for fingerprints occur with equal probability.

Addition and xor however are particularly well-behaved in that regard, as they form finite commutative groups:

\begin{proposition}
Let $\mathcal{G} \defeq (G, \groupaddsym, \neutraladd)$ be a finite commutative group, i.e. a group with a finite universe such that for all $x, y \in G$: $\groupadd{x}{y} = \groupadd{y}{x}$. Let $U$ be a set and let $\fun{f}{U}{G}$ be a hash function.

Then $\lift{\f}{\mathcal{G}}$ is a hash function as well.

\begin{proof}
We first show that for randomly chosen $x, y, z \in G$ the probability that $\groupadd{x}{y} = z$ is $\frac{1}{\abs{G}}$.

For $x, z \in G$ there is $y \in G$ such that $\groupadd{x}{y} = z$, namely $y \defeq \groupsubtract{z}{x}$ (because $\groupadd{x}{\groupsubtract{z}{x}} \overset{\text{commutativity}}= \groupadd{\groupadd{x}{\inverseadd{x}}}{z} = z$). As $G$ is finite, this $y$ has to be unique, since otherwise that would not be enough elements left that can be added to $x$ to result in all of the $\abs{G} - 1$ possible remaining $z'$. Thus for any fixed $x, z$ the probability that a randomly chosen $y$ satisfies $\groupadd{x}{y} = z$ is $\frac{1}{\abs{G}}$.

Computing $\lift{\f}{\mathcal{G}}(S)$ consists of repeatedly adding group elements which by themselves are distributed uniformly at random if $S$ was chosen randomly and $\f$ is a high quality hash function. Thus the accumulated value after every step is any given $z \in G$ with probability $\frac{1}{\abs{G}}$, as is in particular the probability for the final result being equal to $z$.
\end{proof}

A more formal proof for xor specifically is given in section 6.2 of \cite{maziarz2021hashing}.
\end{proposition}

By the same argument, knowing the fingerprint for some set does not provide any information about the fingerprints for sets that differ by even only a single value. In conclusion, high quality fingerprints can be achieved by choosing any transitive and commutative operation for the monoid, for example xor or addition modulo $2^k$, as long as values are mapped into the monoid with a high quality hash function.

While multiplication does not form a group when performed on the numbers in $\interval{0}{2^k}{\N}$, there still are groups based on multiplication modulo some number, e.g. $\Z_n^\ast$, the group yielded by multiplication modulo $n$ on the set $\set{x \in \interval{0}{n}{\N} \mid \text{$x$ is coprime to $n$}}$. In the following, when talking about multiplication, we will assume that the universe is chosen such that multiplication forms a group.

\section{Cryptographically Secure Fingerprints}
\label{crypto}

In the protocols for synchronizing data structures, fingerprints of sets are used for probabilistic equality checking: sets with equal fingerprints are assumed to be equal. Synchronization can thus become faulty if it involves unequal sets with equal fingerprints. If the universe of possible fingerprints is chosen large enough, and the distribution of fingerprints of randomly chosen sets is random within that universe, the probability for this occurring becomes negligible.

Random distribution of input sets is however a very strong assumption. What if a malicious party can influence the sets to be fingerprinted, with the goal of causing fingerprint collisions and consequently triggering faulty behavior of the system? Cryptographically secure fingerprints are an answer to this problem, being chosen such that it is computationally infeasible for an adversary to find inputs that lead to faulty synchronization.

\subsection{General Considerations}

A typical definition of cryptographically secure hash functions is the following~\cite{menezes2018handbook}:

\begin{definition}
A \defined{secure hash function} is a hash function $\fun{\h}{U}{D}$ that satisfies three additional properties:

\begin{description}
  \item[pre-image resistance:] Given $d \in D$, it is computationally infeasible to find a $u \in U$ such that $\h(u) = d$.
  \item[second pre-image resistance:] Given $u \in U$ it is computationally infeasible to find a $u' \in U, u' \neq u$ such that $\h(u) = \h(u')$.
  \item[collision resistance:] It is computationally infeasible to find $u, v \in U, u ~= v$ such that $\h(u) = \h(v)$.
\end{description}
\end{definition}

Pre-image resistance has no influence on the vulnerability of the protocol to malicious actors, so all of the following discussion will focus on collision resistance only.

Since $\lift{\f}{\mathcal{M}}(\set{u}) = \f(u)$, $\f$ must necessarily be collision resistant if $\lift{\f}{\mathcal{M}}$ is to be collision resistant. This alone is unfortunately not sufficient, we will see a specific counterexamples in the following subsections. Choosing a secure hash function $\f$ always comes with a performance cost, insecure hash functions usually take less time and less space to compute. If the synchronization protocol is only being run in a trusted environment, an insecure hash function might be preferable.

Whether a hash function is secure is not a binary dichotomy, but depends on what is considered ``feasible'' for an adversary. Greater security can usually be obtained at the cost of longer digests and longer computation times. Before presenting options for secure hash functions, we thus examine the impact of hash collisions first.

We can generally distinguish between malicious actors in two different positions: those who can actively impact the contents of the data structure to be synchronized, and those who passively relay updates and need to search for a collision within the available data. As a set of size $n$ has $2^n$ subsets, if fingerprints are bit strings of length $k$, then by the pigeonhole principle a fingerprint collision can be found within any set of size at least $k + 1$.

An attack against the fingerprinting scheme by an active adversary can involve computing many fingerprints and adding the required items to the set once a collision has been found. Such an attack is not usable by the passive adversary. We will primarily focus on discussing active adversaries, as they are strictly more powerful than passive ones. Yet it should be kept in mind that passive adversaries can be more common in certain settings, particularly in peer-to-peer systems: if a node is interested in synchronizing a data structure, it probably trusts the source of the data, otherwise it would have little reason for expending resources on synchronization. The data may however be synchronized not with the original source but with completely untrusted nodes. Additionally, an active adversary that does not want to risk detection by adding suspicious items to the data structure is restricted to the operations of a passive adversary.

Fingerprint collisions result in parts of the data structure not being synchronized, so information is being withheld from one or both of the synchronizing nodes. When a malicious node synchronizes with an honest one, the malicious node can withhold arbitrary information by simply pretending not to have certain data, which does not require finding collisions at all.

So the cases in which a malicious node can do actual damage by finding a collision are those where it supplies data to two different, honest nodes such that these two nodes perform faulty synchronization amongst each other. Specifically: let $\mathcal{M}$ be a malicious node, $\mathcal{A}$ and $\mathcal{B}$ be honest nodes, then a successful attack consists of $\mathcal{M}$ crafting sets $X_A, X_B$ and sending these to $\mathcal{A}$ and $\mathcal{B}$ respectively, so that when $\mathcal{A}$ and $\mathcal{B}$ then run the synchronization protocol, they end up with different data structures. A passive adversary does not craft $X_A, X_B$ but must find them as subsets of some set $X$ supplied by an honest node. As we assume the underlying hash function $\f$ to be secure, at least one non-singleton set has to be involved in a collision.

There are some qualitative arguments that even if an adversary finds a fingerprint collision, the impact is rather low. Let $S_A \subseteq X_A$ and $S_B \subseteq X_B$ be nonequal sets with the same fingerprint. To have any impact on the correctness of a particular protocol run, their two fingerprints need to actually be compared during that run. For that to happen, they need to be of the form $\interval{x_A}{y_A}{X_A}$ and $\interval{x_B}{y_B}{X_B}$ respectively. The fingerprints of these intervals can then be compared if one of the nodes sends the fingerprint for the interval $\interval{\min(x_A, x_B)}{\max(y_A, y_B)}{X_i}$. That alone is still not sufficient, as any item within that interval that is not part of the sets would change the fingerprint. So the two intervals actually need to be of the form $\interval{\min(x_A, x_B)}{\max(y_A, y_B)}{X_A}$ and $\interval{\min(x_A, x_B)}{\max(y_A, y_B)}{X_B}$, or simplified: there have to be $x, y \in U$ such that $S_A = \interval{x}{y}{X_A}$ and $S_B = \interval{x}{y}{X_B}$.

If the adversary has found such sets, that is still no guarantee that the interval $\interval{x}{y}{X_i}$ is being compared during the synchronization session of $\mathcal{A}$ and $\mathcal{B}$. For a set containing $n$ items, there are $n^2 - (n - 1) \in \complexity{n^2}$ distinct intervals, but only $\complexity{n}$ are compared in a given protocol run, since the worst-case message complexity is $\complexity{n}$ (see \cref{communication-complexity}). These numbers should only be considered as rough guidelines, they gloss over details such as the fact that there are more intervals containing roughly $\frac{n}{2}$ items then there are intervals containing almost all or almost no items. Yet they demonstrate that finding suitable colliding intervals still does not guarantee that a particular pair of nodes will be affected. In particular, there is no need for $\mathcal{A}$ and $\mathcal{B}$ to choose the interval boundaries that occur in a protocol run deterministically (see \cref{random-boundaries}). They can even perform multiple randomized protocol runs in parallel, while keeping track of item transmissions and sending every item at most once across all these protocol runs.

Another factor mitigating the impact of an adversary finding fingerprint collisions is the communication with other, non-colluding nodes. A fourth party could send some $u \in U, x \preceq u \prec y$ to $\mathcal{A}$ or $\mathcal{B}$ before $\mathcal{A}$ and $\mathcal{B}$ synchronize, disrupting the collision.

Finally, in systems where nodes repeatedly synchronize with different other nodes, a single fingerprint collision in a single synchronization session would merely delay propagation of information rather than stop it completely (note that this does not hold for collisions of two singleton sets). Since the attack model requires both $\mathcal{A}$ and $\mathcal{B}$ to synchronize with more than one node in total, this might apply to many affected settings. Peer-to-peer systems communicating on a random overlay network in particular fall into this category. A malicious actor with enough control over the communication of other nodes to guarantee a tangible benefit from fingerprint collisions can likely disrupt operation of the network more effectively by exercising that control than by sabotaging synchronization.

All of these arguments are however purely qualitative and should as such be taken into account with caution, they are not a substitute for quantitative cryptographic analysis. A strong attacker might be able to find many pairs of sets of colliding fingerprints, or many sets that all share the same fingerprint, and none of the above arguments consider these cases.

In a system with a consensus mechanism among all participating nodes, the choice of $\f$ can periodically be changed, with the frequency being some function of the time it takes to find a viable collision, the cost of rebuilding all auxiliary data structures, and the general level of paranoia among the participating nodes. The $\complexity{n}$ cost of rebuilding the data structures is then being amortized over the number of synchronization sessions occurring between rebuilds, which is still an improvement over protocols that need to perform $\complexity{n}$ computations per synchronization session.

\subsection{Attacking Specific Monoids}

Multiplication, addition and xor as a way of combining hashes have been studied in \cite{bellare1997new}, in the context of hashing sequences, of which our ordered sets are a special case. Their setting requires not only associativity but also commutativity. They thoroughly break xor by reducing the problem of finding a collision to that of solving a system of linear equations. We will not restate the full attack, but we will describe a weaker but simpler mechanism based on similar ideas that allows finding subsets whose fingerprint is a specific target value. This can be used as an optimization in a trusted setting (see \cref{subset-checks}).

The main observation is that xor is the additive operation in $\mathds{F}_2$, the finite field on two elements $\set{0, 1}$. A fingerprint can be interpreted as a vector $\begin{bmatrix}b_1 & b_2 & \ldots & b_k\end{bmatrix} \in \set{0, 1}^{1 \times k}$. The fingerprint of a set $S = \set{s_1, s_2, \ldots, s_n}$ can thus be computed as the sum (within $\mathds{F}_2$, i.e. as the xor) of the vectors corresponding to $s_1, s_2, \ldots, s_n$. The fingerprint of some $S' \subseteq S$ can also be regarded as the sum over all vectors, but each vector being first multiplied with a coefficient of $1$ if $s_i \in S'$ and coefficient $0$ if $s_i \notin S'$. In other words, the fingerprint of $S'$ is a linear combination of the hashes of the items in $S$. 

This enables us to efficiently find subsets whose fingerprint are a particular vector. Let $S = \set{s_1, s_2, \ldots, s_n}$ be a set of items, and let $b = \begin{bmatrix}b_1 & b_2 & \ldots & b_k\end{bmatrix} \in \set{0, 1}^{1 \times k}$ be the target fingerprint. For $0 < i \leq n$ define $a_i = \begin{bmatrix}a_{i, 1} & a_{i, 2} & \ldots & a_{i, k}\end{bmatrix} \in \set{0, 1}^{1 \times k}$ to be $\f(s_i)$ interpreted as a vector over $\mathds{F}_2$. The coefficients for linear combinations of the $a_i$ equal to $b$ are the solutions to the following system of linear equations over $\mathds{F}_2$:

\[
\begin{bmatrix}
a_{1, 1} & a_{1, 2} & \ldots & a_{1, k}\\
a_{2, 1} & a_{2, 2} & \ldots & a_{2, k}\\
\vdots & \vdots & \ddots & \vdots \\
a_{n, 1} & a_{n, 2} & \ldots & a_{n, k}\\
\end{bmatrix} \cdot \begin{bmatrix}
x_1\\
x_2\\
\vdots\\
x_n
\end{bmatrix} = \begin{bmatrix}
b_1\\
b_2\\
\vdots\\
b_n
\end{bmatrix}
\]

Solutions can be found by gaussian elimination in $\complexity{n^3}$ time, an exponential improvement over brute forcing by computing the fingerprints of all subsets.

As xor admits polynomial-time attacks by solving linear equations, the authors of \cite{bellare1997new} next consider addition and multiplication. They unify parts of their discussion by relating the hardness of finding collisions to solving the balance problem: in a commutative group $(G, \groupaddsym, \neutraladd)$, given a set of group elements $S = \set{s_1, s_2, \ldots, s_n}$, find disjoint, nonempty subsets $S_0 = \set{s_{0, 0}, s_{0, 1}, \ldots, s_{0, k}} \subseteq S, S_1 = \set{s_{1, 0}, s_{1, 1}, \ldots, s_{1, l}} \subseteq S$ such that $s_{0, 0} \groupaddsym s_{0, 1}  \groupaddsym \ldots  \groupaddsym s_{0, k} = s_{1, 0}  \groupaddsym s_{1, 1}  \groupaddsym \ldots  \groupaddsym s_{1, l}$. They then reduce the hardness of the balance problem to other problems.

For addition, the balance problem is as hard as subset sum, which was at the time of publication conjectured to be sufficiently hard. Wagner showed however in \cite{wagner2002generalized} how to solve the balance problem in subexponential time for addition. To give an impression for the impact of this attack, consider \cite{mihajloska2015reviving} which uses addition for combining SHA-3~\cite{dworkin2015sha} digests, producing fingerprints of length between $2688$ and $4160$ or $6528$ to $16512$ bits to achieve security levels of $128$ or $256$ bit respectively against Wagner's attack. \cite{lyubashevsky2005parity} gives an improvement over Wagner's attack finding collisions in $\complexity{2^{n^\epsilon}}$ for arbitrary $\epsilon < 1$, further weakening addition as a choice of monoid operation.

For multiplication, the balance problem is as hard as the discrete logarithm problem in the group. This is a more ``traditional'' hardness assumption than subset sum, there are groups for which no efficient algorithm is known. The main drawback is that multiplication is less efficient to compute that addition. \cite{stanton2010fastad} includes a comparison between the performance of addition and multiplication for incremental hashing, the additive hash outperforms the multiplicative one by two orders of magnitude, even though it uses longer digests to account for Wagner's attack.

For our context in which fingerprints are frequently sent over the network, longer computation time might still be preferable over longer hashes. Fingerprints based on multiplication nevertheless need larger digests than traditional, non-incremental hash functions. \cite{maitin2017elliptic} suggests fingerprints of $3200$ bit to achieve $128$ bit security, and motivated by this uses binary elliptic curves as the underlying group to achieve more compact fingerprints - fingerprints of length $2k$ bits give $\complexity{2^k}$ security.

\cite{bellare1997new} also proposes a fourth monoid based on lattices. \cite{lewi2019securing} give a specific instantiation providing 200 bits of security with fingerprints of size $16 \cdot 1024 = 16384$ bit.

All of the preceeding monoid operations are commutative, which our approach to fingerprint computation does not require. A typical associative but not commutative operation is matrix multiplication. Study of a family of hash functions based on multiplication of invertible matrices was initiated in \cite{zemor1991hash}, whose security is related to solving hard graph problems on the Caley graph of the group. \cite{petit2011rubik} gives a good overview about the general principle and the security aspects of Caley hash functions.

While \cite{tillich1994hashing}, an improvement over the originally proposed scheme, has been successfully attacked in \cite{grassl2011cryptanalysis}\cite{petit2010preimages}, there are several modifications~\cite{petit2009graph}\cite{bromberg2017navigating}\cite{sosnovski2016cayley} for which no attacks are currently known, and \cite{mullan2016text} showed random self-reducibility for Caley hash functions.

Whereas the schemes based on commutative groups operate on two bitstrings at a time, the Caley hash functions operate on two individual bits at a time. Attacks thus assume that the manipulated bits can be freely chosen, using this to e.g. craft palindromic inputs. This may mean that such attacks are hard to apply to our setting where the input bits are produced in non-reorderable batches by another, cryptographically secure hash function. After finding a bit sequence that yields a Caley hash collision, an attacker still needs to find items for which the concatenation of their hashes is the desired it sequence.

Alternatively, one can forgo the additional hash function and apply the Caley hash function directly to the encodings of items. This simplifies the overall scheme and removes reliance on a second hash function. On the other hand, if one expects the additional hash function to be harder to attack than the Caley hash, then making the attacks against the Caley hash function less flexible might overall make attacks more difficult. Furthermore, Caley hash functions are usually slower than typical choices for the hash function from items to bitstrings, so using both can produce significant speedups if items have long encodings.

Aside from Caley hashes, we are not aware of any non-commutative monoids used for hashing. We would further like to note that any hashes based on groups still have more structure than required for our designs, as we don't need existence of inverse elements. Strictly speaking our designs do not even require a neutral element: the fingerprint of the empty set never needs to be exchanged in a protocol run, so we could have just as well defined fingerprints for all nonempty sets without relying on the neutral element to do so. The presentation we chose is merely more elegant. And finally, we do not require pre-image resistance for our fingerprints. Suitable hash functions could thus be located in a more general design space than studied in any literature we know of.
